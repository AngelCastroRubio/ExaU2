{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfLF0ztBX1djoO6fpkJZcJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AngelCastroRubio/ExaU2/blob/main/Etapa%204%20Ex2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parte 1, correspondiente a la Importación de datos y módulos básicos, Visualización y Análisis Exploratorio del Conjunto de Datos y a las Estadísticas de Resumen y Coeficiente de Variación"
      ],
      "metadata": {
        "id": "9_ZsSJiMH3xM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SqXlugyIHvzr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ee3a056-6d52-46d4-ec9b-e27273289ce5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      id  gender   age  hypertension  heart_disease ever_married  \\\n",
            "0   9046    Male  67.0             0              1          Yes   \n",
            "1  51676  Female  61.0             0              0          Yes   \n",
            "2  31112    Male  80.0             0              1          Yes   \n",
            "3  60182  Female  49.0             0              0          Yes   \n",
            "4   1665  Female  79.0             1              0          Yes   \n",
            "\n",
            "       work_type Residence_type  avg_glucose_level   bmi   smoking_status  \\\n",
            "0        Private          Urban             228.69  36.6  formerly smoked   \n",
            "1  Self-employed          Rural             202.21   NaN     never smoked   \n",
            "2        Private          Rural             105.92  32.5     never smoked   \n",
            "3        Private          Urban             171.23  34.4           smokes   \n",
            "4  Self-employed          Rural             174.12  24.0     never smoked   \n",
            "\n",
            "   stroke  \n",
            "0       1  \n",
            "1       1  \n",
            "2       1  \n",
            "3       1  \n",
            "4       1  \n",
            "id                     int64\n",
            "gender                object\n",
            "age                  float64\n",
            "hypertension           int64\n",
            "heart_disease          int64\n",
            "ever_married          object\n",
            "work_type             object\n",
            "Residence_type        object\n",
            "avg_glucose_level    float64\n",
            "bmi                  float64\n",
            "smoking_status        object\n",
            "stroke                 int64\n",
            "dtype: object\n",
            "id                   5110\n",
            "gender                  3\n",
            "age                   104\n",
            "hypertension            2\n",
            "heart_disease           2\n",
            "ever_married            2\n",
            "work_type               5\n",
            "Residence_type          2\n",
            "avg_glucose_level    3979\n",
            "bmi                   418\n",
            "smoking_status          4\n",
            "stroke                  2\n",
            "dtype: int64\n",
            "                 id          age  hypertension  heart_disease  \\\n",
            "count   5110.000000  5110.000000   5110.000000    5110.000000   \n",
            "mean   36517.829354    43.226614      0.097456       0.054012   \n",
            "std    21161.721625    22.612647      0.296607       0.226063   \n",
            "min       67.000000     0.080000      0.000000       0.000000   \n",
            "25%    17741.250000    25.000000      0.000000       0.000000   \n",
            "50%    36932.000000    45.000000      0.000000       0.000000   \n",
            "75%    54682.000000    61.000000      0.000000       0.000000   \n",
            "max    72940.000000    82.000000      1.000000       1.000000   \n",
            "\n",
            "       avg_glucose_level          bmi       stroke  \n",
            "count        5110.000000  4909.000000  5110.000000  \n",
            "mean          106.147677    28.893237     0.048728  \n",
            "std            45.283560     7.854067     0.215320  \n",
            "min            55.120000    10.300000     0.000000  \n",
            "25%            77.245000    23.500000     0.000000  \n",
            "50%            91.885000    28.100000     0.000000  \n",
            "75%           114.090000    33.100000     0.000000  \n",
            "max           271.740000    97.600000     1.000000  \n",
            "Desviacion estandar:\n",
            "id                   21161.721625\n",
            "age                     22.612647\n",
            "hypertension             0.296607\n",
            "heart_disease            0.226063\n",
            "avg_glucose_level       45.283560\n",
            "bmi                      7.854067\n",
            "stroke                   0.215320\n",
            "dtype: float64\n",
            "\n",
            "Coeficiente de variacion:\n",
            "id                   0.579490\n",
            "age                  0.523119\n",
            "hypertension         3.043494\n",
            "heart_disease        4.185442\n",
            "avg_glucose_level    0.426609\n",
            "bmi                  0.271831\n",
            "stroke               4.418813\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "#Imports de la etapa 1\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# Imports de la Etapa 3\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "#Imports Etapa 4\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, balanced_accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_curve\n",
        "url = 'https://raw.githubusercontent.com/AngelCastroRubio/ExaU2/main/healthcare-dataset-stroke-data.csv'\n",
        "data = pd.read_csv(url)\n",
        "# Dataset is now stored in a Pandas Dataframe\n",
        "\n",
        "# Visualización de las primeras filas del conjunto de datos\n",
        "print(data.head())\n",
        "\n",
        "# Verificación de tipos de datos\n",
        "print(data.dtypes)\n",
        "\n",
        "# Análisis de valores únicos\n",
        "print(data.nunique())\n",
        "\n",
        "#----Estadísticas de Resumen y Coeficiente de Variación----\n",
        "# Estadísticas descriptivas\n",
        "print(data.describe())\n",
        "\n",
        "# Seleccionar solo columnas numéricas\n",
        "data_numeric = data.select_dtypes(include=[np.number])\n",
        "\n",
        "# Calcular la desviación estándar y el coeficiente de variación\n",
        "std = data_numeric.std()\n",
        "mean = data_numeric.mean()\n",
        "cv = std / mean\n",
        "\n",
        "# Mostrar los resultados\n",
        "print(\"Desviacion estandar:\")\n",
        "print(std)\n",
        "print(\"\\nCoeficiente de variacion:\")\n",
        "print(cv)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Etapa 2: Preparación de los Datos. Se siguen los siguientes pasos: Eliminación de Variables Redundantes, Comprobación e Imputación de Valores Perdidos, Identificación y Limitación de Valores Atípicos, Codificación de Características, Importancia de las Características y Eliminación de la Multicolinealidad"
      ],
      "metadata": {
        "id": "h9EaJCe_t2yT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------------------Etapa 2: Preparación de los Datos--------------------------\n",
        "cols_objeto = data.select_dtypes(include=['object']).columns\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for col in cols_objeto:\n",
        "    data[col] = label_encoder.fit_transform(data[col])\n",
        "\n",
        "# Eliminamos la variable ID del DataSet\n",
        "data.drop('id', axis=1, inplace=True)\n",
        "\n",
        "# Realizamos la Comprobación e Imputación a BMI usando la mediana\n",
        "median_bmi = data['bmi'].median()\n",
        "data.loc[:, 'bmi'] = data['bmi'].fillna(median_bmi)\n",
        "\n",
        "# Limitamos los valores atípicos de nuestra muestra con los percentiles\n",
        "q_low = data['avg_glucose_level'].quantile(0.05)\n",
        "q_hi = data['avg_glucose_level'].quantile(0.95)\n",
        "data = data[(data['avg_glucose_level'] >= q_low) & (data['avg_glucose_level'] <= q_hi)]\n",
        "\n",
        "q_low = data['bmi'].quantile(0.05)\n",
        "q_hi = data['bmi'].quantile(0.95)\n",
        "data = data[(data['bmi'] >= q_low) & (data['bmi'] <= q_hi)]\n",
        "\n",
        "# Codificamos las caracteristicas actuales y las usamos numericamente\n",
        "data_encoded = pd.get_dummies(data)\n",
        "print(data_encoded.columns)\n",
        "\n",
        "# Renombrar la columna 'stroke' a 'ictus'\n",
        "data_encoded.rename(columns={'stroke': 'ictus'}, inplace=True)\n",
        "\n",
        "# Separar las características (X) y la variable objetivo (y)\n",
        "X = data_encoded.drop('ictus', axis=1)\n",
        "y = data_encoded['ictus']\n",
        "\n",
        "# Importancia de las características\n",
        "from xgboost import XGBClassifier\n",
        "model = XGBClassifier()\n",
        "model.fit(X, y)\n",
        "feature_importances = model.feature_importances_\n",
        "print(X.columns)\n",
        "\n",
        "# Eliminación de la multicolinealidad\n",
        "corr_matrix = X.corr().abs()\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
        "X = X.drop(to_drop, axis=1)\n",
        "print(X.columns)\n",
        "\n",
        "# Mostrar las primeras filas del conjunto de datos con la nueva columna 'ictus'\n",
        "print(X.head())"
      ],
      "metadata": {
        "id": "dUERBr45t42G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ecfad54-4c79-4fbf-9dda-b1b899de7f19"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['gender', 'age', 'hypertension', 'heart_disease', 'ever_married',\n",
            "       'work_type', 'Residence_type', 'avg_glucose_level', 'bmi',\n",
            "       'smoking_status', 'stroke'],\n",
            "      dtype='object')\n",
            "Index(['gender', 'age', 'hypertension', 'heart_disease', 'ever_married',\n",
            "       'work_type', 'Residence_type', 'avg_glucose_level', 'bmi',\n",
            "       'smoking_status'],\n",
            "      dtype='object')\n",
            "Index(['gender', 'age', 'hypertension', 'heart_disease', 'ever_married',\n",
            "       'work_type', 'Residence_type', 'avg_glucose_level', 'bmi',\n",
            "       'smoking_status'],\n",
            "      dtype='object')\n",
            "   gender   age  hypertension  heart_disease  ever_married  work_type  \\\n",
            "1       0  61.0             0              0             1          3   \n",
            "2       1  80.0             0              1             1          2   \n",
            "3       0  49.0             0              0             1          2   \n",
            "4       0  79.0             1              0             1          3   \n",
            "5       1  81.0             0              0             1          2   \n",
            "\n",
            "   Residence_type  avg_glucose_level   bmi  smoking_status  \n",
            "1               0             202.21  28.1               2  \n",
            "2               0             105.92  32.5               2  \n",
            "3               1             171.23  34.4               3  \n",
            "4               0             174.12  24.0               2  \n",
            "5               1             186.21  29.0               1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parte 3"
      ],
      "metadata": {
        "id": "jHgrnNsr49cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dividir el conjunto de datos en entrenamiento y prueba (70% entrenamiento, 30% prueba)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Entrenar modelo de regresión logística\n",
        "model = LogisticRegression(max_iter=1000)  # Ajusta el valor de max_iter según sea necesario\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Hacer predicciones\n",
        "y_pred_train = model.predict(X_train)\n",
        "y_pred_test = model.predict(X_test)\n",
        "\n",
        "# Calcular precisión\n",
        "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
        "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "# Crear DataFrame con los resultados\n",
        "results = pd.DataFrame({\n",
        "    'Conjunto de Datos': ['Entrenamiento', 'Prueba'],\n",
        "    'Precisión': [accuracy_train, accuracy_test]\n",
        "})\n",
        "\n",
        "# Mostrar los resultados\n",
        "print(results)"
      ],
      "metadata": {
        "id": "SHquoYojHzdG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eef47c51-8acf-4dd3-e1d5-39f0d1dffc76"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Conjunto de Datos  Precisión\n",
            "0     Entrenamiento   0.956252\n",
            "1            Prueba   0.945382\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Etapa 4"
      ],
      "metadata": {
        "id": "K-C2TJPzI7F3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------Etapa 4: Evaluacion de modelos------------------------------\n",
        "# Métricas de rendimiento para los datos de entrenamiento\n",
        "y_pred_train = model.predict(X_train)\n",
        "precision_train = precision_score(y_train, y_pred_train)\n",
        "recall_train = recall_score(y_train, y_pred_train)\n",
        "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
        "f1_train = f1_score(y_train, y_pred_train)\n",
        "balanced_accuracy_train = balanced_accuracy_score(y_train, y_pred_train)\n",
        "\n",
        "# Tasa de verdaderos positivos (Recall) para los datos de entrenamiento\n",
        "tp_train = ((y_train == 1) & (y_pred_train == 1)).sum()\n",
        "fn_train = ((y_train == 1) & (y_pred_train == 0)).sum()\n",
        "tpr_train = tp_train / (tp_train + fn_train)\n",
        "\n",
        "# Tasa de verdaderos negativos (Especificidad) para los datos de entrenamiento\n",
        "tn_train = ((y_train == 0) & (y_pred_train == 0)).sum()\n",
        "fp_train = ((y_train == 0) & (y_pred_train == 1)).sum()\n",
        "tnr_train = tn_train / (tn_train + fp_train)\n",
        "\n",
        "# Métricas de rendimiento para los datos de prueba\n",
        "y_pred_test = model.predict(X_test)\n",
        "precision_test = precision_score(y_test, y_pred_test, zero_division=0)\n",
        "recall_test = recall_score(y_test, y_pred_test)\n",
        "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
        "f1_test = f1_score(y_test, y_pred_test)\n",
        "balanced_accuracy_test = balanced_accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "# Matriz de confusión para los datos de prueba\n",
        "conf_matrix_test = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "print(\"\\nMatriz de confusión para los datos de prueba:\")\n",
        "print(conf_matrix_test)\n",
        "\n",
        "# Análisis de si el modelo está sobreajustado\n",
        "if recall_train > recall_test:\n",
        "    print(\"\\nEl modelo está sobreajustado (overfitting) ya que el recall en los datos de entrenamiento es mayor que en los datos de prueba.\")\n",
        "else:\n",
        "    print(\"\\nEl modelo no está sobreajustado (overfitting).\")\n",
        "\n",
        "# Tasa de verdaderos positivos (Recall) para los datos de prueba\n",
        "tp_test = ((y_test == 1) & (y_pred_test == 1)).sum()\n",
        "fn_test = ((y_test == 1) & (y_pred_test == 0)).sum()\n",
        "tpr_test = tp_test / (tp_test + fn_test)\n",
        "\n",
        "# Tasa de verdaderos negativos (Especificidad) para los datos de prueba\n",
        "tn_test = ((y_test == 0) & (y_pred_test == 0)).sum()\n",
        "fp_test = ((y_test == 0) & (y_pred_test == 1)).sum()\n",
        "tnr_test = tn_test / (tn_test + fp_test)\n",
        "\n",
        "print(\"Métricas de rendimiento para los datos de entrenamiento:\")\n",
        "print(f\"Precisión: {precision_train}\")\n",
        "print(f\"Recall (Tasa de verdaderos positivos): {recall_train}\")\n",
        "print(f\"Especificidad (Tasa de verdaderos negativos): {tnr_train}\")\n",
        "print(f\"Precisión equilibrada: {balanced_accuracy_train}\")\n",
        "print(f\"Puntuación F1: {f1_train}\")\n",
        "\n",
        "print(\"\\nMétricas de rendimiento para los datos de prueba:\")\n",
        "print(f\"Precisión: {precision_test}\")\n",
        "print(f\"Recall (Tasa de verdaderos positivos): {recall_test}\")\n",
        "print(f\"Especificidad (Tasa de verdaderos negativos): {tnr_test}\")\n",
        "print(f\"Precisión equilibrada: {balanced_accuracy_test}\")\n",
        "print(f\"Puntuación F1: {f1_test}\")\n",
        "\n",
        "# Calcular probabilidades predichas\n",
        "y_prob_test = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calcular la curva ROC\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob_test)\n",
        "\n",
        "# Encontrar el umbral óptimo\n",
        "optimal_threshold_idx = np.argmax(tpr - fpr)\n",
        "optimal_threshold = thresholds[optimal_threshold_idx]\n",
        "\n",
        "# Convertir probabilidades predichas en clases binarias con el umbral optimo\n",
        "y_pred_test_optimal = (y_prob_test >= optimal_threshold).astype(int)\n",
        "\n",
        "# Calcular métricas de rendimiento con el umbral óptimo\n",
        "precision_test_optimal = precision_score(y_test, y_pred_test_optimal)\n",
        "recall_test_optimal = recall_score(y_test, y_pred_test_optimal)\n",
        "accuracy_test_optimal = accuracy_score(y_test, y_pred_test_optimal)\n",
        "f1_test_optimal = f1_score(y_test, y_pred_test_optimal)\n",
        "balanced_accuracy_test_optimal = balanced_accuracy_score(y_test, y_pred_test_optimal)\n",
        "\n",
        "# Comparar los resultados con el umbral por defecto\n",
        "print(\"\\nMétricas de rendimiento para los datos de prueba con umbral optimo:\")\n",
        "print(f\"Precision: {precision_test_optimal}\")\n",
        "print(f\"Recall (Tasa de verdaderos positivos): {recall_test_optimal}\")\n",
        "print(f\"Precision equilibrada: {balanced_accuracy_test_optimal}\")\n",
        "print(f\"Puntuacion F1: {f1_test_optimal}\")\n",
        "\n",
        "# Comparar con los resultados anteriores\n",
        "print(\"\\nComparacion con umbral por defecto:\")\n",
        "print(f\"Precision: {precision_test}\")\n",
        "print(f\"Recall (Tasa de verdaderos positivos): {recall_test}\")\n",
        "print(f\"Precision equilibrada: {balanced_accuracy_test}\")\n",
        "print(f\"Puntuacion F1: {f1_test}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNwSKhPtI-6j",
        "outputId": "86ea17f2-20b0-4872-d1b7-33052fc44e22"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Matriz de confusión para los datos de prueba:\n",
            "[[1177    0]\n",
            " [  68    0]]\n",
            "\n",
            "El modelo está sobreajustado (overfitting) ya que el recall en los datos de entrenamiento es mayor que en los datos de prueba.\n",
            "Métricas de rendimiento para los datos de entrenamiento:\n",
            "Precisión: 1.0\n",
            "Recall (Tasa de verdaderos positivos): 0.0078125\n",
            "Especificidad (Tasa de verdaderos negativos): 1.0\n",
            "Precisión equilibrada: 0.50390625\n",
            "Puntuación F1: 0.015503875968992248\n",
            "\n",
            "Métricas de rendimiento para los datos de prueba:\n",
            "Precisión: 0.0\n",
            "Recall (Tasa de verdaderos positivos): 0.0\n",
            "Especificidad (Tasa de verdaderos negativos): 1.0\n",
            "Precisión equilibrada: 0.5\n",
            "Puntuación F1: 0.0\n",
            "\n",
            "Métricas de rendimiento para los datos de prueba con umbral optimo:\n",
            "Precision: 0.13176470588235295\n",
            "Recall (Tasa de verdaderos positivos): 0.8235294117647058\n",
            "Precision equilibrada: 0.7550102453895746\n",
            "Puntuacion F1: 0.22718052738336714\n",
            "\n",
            "Comparacion con umbral por defecto:\n",
            "Precision: 0.0\n",
            "Recall (Tasa de verdaderos positivos): 0.0\n",
            "Precision equilibrada: 0.5\n",
            "Puntuacion F1: 0.0\n"
          ]
        }
      ]
    }
  ]
}